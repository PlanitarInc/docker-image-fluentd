# XXX Consider working directly with /var/lib/docker/containers/

<source>
  type tail
  path /tmp/lib/docker/containers/*/*-json.log
  pos_file /var/log/fluentd-docker.pos
  time_format %Y-%m-%dT%H:%M:%S
  tag raw.docker.*
  format json
  time_key null # Don't touch the time field!
</source>

<match raw.docker.tmp.lib.docker.containers.*.*.log>
  type docker_format
  docker_containers_path /tmp/lib/docker/containers
  container_id "${tag_parts[-3]}"
  tag "raw.docker.file.${name}"
</match>

# Turn this on during debugging, when your fluentd runs in docker container
# itself and streams everything to stdout, in order to avoid infinite loop.
#  <match raw.docker.file.fluentd>
#    type null
#  </match>

<match raw.docker.file.**>
  type record_reformer
  enable_ruby true

  tag docker.normalized
  remove_keys full_id
</match>

# Turn this on when you want to stream the logs from logspout
#  <source>
#    type udp
#    bind 0.0.0.0
#    port "#{ENV['DOCKER_UDP_PORT'] || 5141}"
#    format json
#    tag raw.docker.udp
#  </source>

<match raw.docker.udp>
  type record_reformer
  enable_ruby true

  tag docker.normalized
  <record>
    stream "${type}"
    log "${data}"
    time "${t = Time.now; t.strftime('%Y-%m-%dT%H:%M:%S.%9N%z')}"
  </record>
  remove_keys type,data
</match>

<match docker.normalized>
  type record_reformer
  enable_ruby true

  tag "s3.docker.${name}.${id}.${stream}"
  remove_keys name,id,stream
</match>

#  <match s3.docker.**>
#    type stdout
#    output_type json
#  </match>

<match null>
  type null
</match>

<match out.docker.**>
  type stdout
  output_type json
</match>

# XXX: `forest` was not intended to work with infinitely growing #tags
<match s3.docker.*.*.*>
  type forest
  subtype s3

  remove_prefix s3.docker
  # original tag = s3.docker.<name>.<id>.<type>
  # hence 
  #  tag = <name>.<id>.<type>
  #  tag_parts:
  #   [0] = container name
  #   [1] = container id
  #   [2] = stream type (stdout or stderr)

  <template>
    # No need to set the keys explicitly. AWS Ruby SDK will read it by itself:
    #  1. From ENV VARS: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
    #  2. If not defined, will try to get temporary ones for EC2 IAM role.
    s3_region "#{ENV['AWS_S3_REGION']}"
    s3_bucket "#{ENV['AWS_S3_BUCKET']}"
    path logs/docker
    s3_object_key_format "%{path}/%{time_slice}_%{index}.%{file_extension}"
    buffer_path "/var/log/fluent/forest-s3/${tag}.buf"
    format json
    # YYYY-MM-DD/<HOSTNAME>/<NAME>-<ID>_<TYPE>.HH
    time_slice_format "%Y-%m-%d/${hostname}/${tag_parts[0]}-${tag_parts[1]}_${tag_parts[2]}.%H:00"
    time_slice_wait 1m
    utc
    use_ssl
    buffer_chunk_limit 256m
  </template>
</match>
